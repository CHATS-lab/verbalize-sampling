\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{figures/intro/intro_teaser.pdf}
    \caption{
    We show that typicality bias in preference data is a fundamental and pervasive cause of \emph{mode collapse}, reducing output diversity. As a solution, we propose Verbalized Sampling (VS), a principled  prompting method that returns distributions of responses. Qualitative examples on multiple tasks demonstrate that VS improves generation diversity and simulation quality, and enables output diversity tuning. %,
    % and enables tunable control over output diversity.
    % , as shown in the story writing example.
    % As shown in the story task, we can also tune the probability in VS to change the diversity level. 
    % As shown in the ``story writing'' example, we can also tune the probability in VS to change the diversity level. 
    % \wyshi{may need to update synthetic data generation depending on experimental results} 
    % \wyshi{need to update the diversity score based on new metrics}
    % \vspace{-2em}
    % When prompted to generate a roll a fair six-sided die, the base model produces an almost uniform distribution over 1 to 6. After RLHF, however, it collapses to frequently output `4,' as shown in Random Number Generation task in the figure. This phenomenon is known as mode collapse. We first provide a theoretical proof of this phenomenon and introduce \ours (VS), a principled prompting method that explicitly query for a distribution of responses with verbalized probabilities, to restore the diversity of the base model. Beyond theory, VS works broadly across multiple tasks. Qualitative examples in the figure illustrate how VS restores the diversity across tasks.
    % For the two tasks in the second row, we also plot the distribution of responses. We see that direct prompting suffers from mode collapse, but \ours can generate more diverse and evenly distributed responses. 
    }
    \label{fig:intro_teaser}
\end{figure}

\section{Introduction}\label{sec:introduction}
% Introduce your topic and contributions.
% \wyshi{add an intro figure that explains everything}
% \wyshi{I am thinking if it's easy to run a fine-tuned model on all the tasks for the VS project? and we can show how close it's to a fine-tuned model? I am thinking of a Figure 2 (a bar plot) with 4 tasks X 3 methods (baseline, fine-tuned, ours). But if it's too much, this could wait until the full paper.}
% RLHF has become a standard practice to ensure alignment with human preferences. However, it introduces \emph{mode collapse}—a significant reduction in output diversity, an unintended consequence: a significant reduction in output diversity~\citep{padmakumar_does_2024,lu2025aihumanityssalieriquantifying,west2025basemodelsbeataligned}.
% This phenomenon, known as \emph{mode collapse} \citep{janus2022modecollapse,omahony2024attributing,kirk2024understandingeffectsrlhfllm}, causes aligned  models to 
% % generate safe but near-duplicate responses (the ``mode'')
% favor a narrow set of responses (the ``mode'') over all plausible outputs.

Post-training alignment methods like RLHF %a standard alignment method,
can unintentionally cause \emph{mode collapse} \citep{janus2022modecollapse,omahony2024attributing,kirk2024understandingeffectsrlhfllm}, whereby the model favors a narrow set of responses (the ``mode'') over all plausible outputs, as shown in Figure~\ref{fig:intro_teaser}. This significantly reduces output diversity~\citep{padmakumar_does_2024,west2025basemodelsbeataligned}
%by making the model favor a narrow set of "mode" responses, sacrificing creativity and variety.
%However, it introduces an unintended consequence: a significant reduction in output diversity~\citep{padmakumar_does_2024,lu2025aihumanityssalieriquantifying,west2025basemodelsbeataligned}.
%This phenomenon, known as \emph{mode collapse} \citep{janus2022modecollapse,omahony2024attributing,kirk2024understandingeffectsrlhfllm}, causes aligned  models to 
%favor a narrow set of responses (the ``mode'') over all plausible outputs. %For instance, as shown in Figure~\ref{fig:intro_teaser}, even if we prompt the model at a high temperature, it keeps generating the same joke. 
% For instance, as shown in Figure~\ref{fig:intro_teaser}\todo{Missing ref}, even at a high-temperature setting, the model keeps generating the exact same joke. 
%\wyshi{maybe also mention the random number generation, "When asked to roll a fair six-sided dice, it will generate 4 xx times out of xx times"}. 
% drastic biases toward particular completions and patterns
and limits LLMs' effectiveness in various applications such as creative writing \citep{lu2025aihumanityssalieriquantifying}, social simulation \citep{anthis2025llmsocialsimulationspromising}, pluralistic alignment \citep{kirk2024prismalignmentdatasetparticipatory}, and synthetic data generation \citep{zhu2025bareleveragingbaselanguage}. %personalized LLM-based agents \citep{shaikh2025creatinggeneralusermodels}.%\cmcomment{Presumably it is not only RLHF but also any kind of post-training from preference pairs such as DPO etc. Wouldn’t it be good to mention that? “RLHF, or other alignment methods like DPO standardly used in post-training LLMs”}% open-ended tasks, such as realistic social simulation, where users expect a range of responses tailored to their individual preferences~\citep{zhang_diverging_2024}. 
% Recent work has shown this lack of diversity undermines efforts toward pluralistic alignment~\citep{sorensen2024roadmappluralisticalignment} and limits the adaptability of LLM-based agent systems \cite{shaikh2025creatinggeneralusermodels} \wyshi{combine these tasks, and select tasks related to the tasks we evaluated later, right now these tasks feel disconnected to the rest of the paper}.
% \wyshi{make it more concise and human-like}
% \jiayicomment{think of defining the "mode collapse" as a new term not the commonly known phenomenon}


% \wyshi{the citation distribution is very uneven, sometimes it's lots of citations in one sentence, but other times it's only one citation for one big claim.}
% \as{Below paragraph reads a little bit like related word, should we axe for more detail on contributions?}



% For instance, it is inadequate to rely on a single reward model to capture diverse human preferences \cite{chakraborty2024maxmin} \citep{xiao2024algorithmic} and the KL-regularized optimization used in RLHF tends to amplify common, majority-style responses.
Existing work often attributes mode collapse to algorithmic causes such as inadequate reward models \citep{chakraborty2024maxmin} or the majority-favoring optimization process \citep{xiao2024algorithmic}. {In this paper, we show that the issue is more fundamental and pervasive: mode collapse is an inherent property of preference data itself}.  We identify \emph{typicality bias}, the human tendency to prefer more typical text, as a fundamental data-level cause for mode collapse. Critically, this means that even with a perfect reward model and optimization process, inherent bias within preference datasets may still drive mode collapse, affecting the majority of alignment methods that rely on reward models. In Section~\ref{sec:typicality}, we formalize this concept with an analytical model, corroborated by empirical verification on preference datasets, to confirm the central role of typicality bias.
% We formalize an analytical model for how this cognitive bias can impact data during post-training (corroborated by empirical verifications on multiple preference datasets) and use our analysis to confirm the role of typicality bias in causing mode collapse.
% show that mode collapse is an inherent, pervasive property of the data, rather than an algorithmic bias that can be patched during training, even if we can design perfect reward models and perfectly maximized the reward, due to the inherent typicality bias in the data, we will still have mode collapse. 



%The issue is compounded by practices that occur even before RLHF: supervised fine-tuning (SFT) narrows the model's vocabulary \citep{li2024entropic}, and rigid chat templates further restrict its creativity \citep{yun2025price}. 
% So, we propose a thought experiment based on our theoretical findings: “what if we had access to a model that perfectly maximized the reward, would still have mode collapse because of the inherence data bias. %I think the other thing here to emphasize the novelty of our work is to pitch the mode collapsed oracle itself as a contribution -- it's really a new way to think about RLHF tuned LLMs and how to prompt them. VS is one algorithm that we recover from the thought experiment, because our focus is diversity, but it's great empirical success suggests this may be an improved direction of thought for LLM prompting aimed at other objectives.


% traces RLHF-driven mode collapse to KL-regularized optimization that amplifies majority modes \ciicatep{xiao2024algorithmic} and to the inadequacy of single-reward modeling under heterogeneous preferences \citep{chakraborty2024maxmin}, while pre-RLHF choices—cross-entropy SFT that narrows token distributions \citep{li2024entropic} and rigid chat templating \citep{yun2025price}—further compress diversity. 
% Our work builds on these existing studies by identifying a pervasive data bias (i.e., \textit{typlity}) that compounds existing algorithmic causes of mode collapse in RLHF. Moreover, we build on this theoretical contribution with practical, inference-time prompting approach, \emph{\ours}, to recover the model's output diversity.


%While these approaches can increase diversity, 
% But these methods often require substantial computational resources, extra care during inference time, or only work on open-source models. 
% Furthermore, training-based methods may undermines the safety benefits achieved by alignment~\citep{qi2024safetyalignmentjusttokens}, and decoding strategies often fail to fully address alignment-induced mode collapse~\citep{yang_how_2025}.
% Prompting-based techniques have emerged as a lightweight alternative~\citep{SummersStay2023BrainstormTS, mehrotra2024enhancingcreativitylargelanguage, wong2024simplestratdiversifyinglanguagemodel, tian2025macgyverlargelanguagemodels}, but many require manual, handcrafted prompts that can be overly prescriptive and complex. 
% by shifting probability mass toward safer but less varied responses. 

% \wyshi{I think the current structure somehow buries our key contribution, since we spent lot of works on existing work.}

% To this end, we first theoretically prove that proves RLHF indeed induces mode collapse by concentrating probability mass on a narrow set of  outputs (the mode). Inspired by the insights from the proof, we also propose a simple yet principled prompting strategy to mitigate mode collapse: we prompt the model to generate multiple candidate responses in a single pass and explicitly verbalize their corresponding probabilities,  as illustrated in Figure~\ref{fig:intro_teaser}. We call this method \textbf{\textit{\ours}} (VS). In this way, we reformulate formulating the same prompt that asks for one instance, to a distributional query that asks for a distribution. As base models have learned factual and diverse knowledge during pretraining, the mode for such a distributional-level query will be the 

% To this end, we identify \textit{typicality bias} -- the human tendency to prefer more typical text -- as a fundamental data-level cause for mode collapse in RLHF. We formalize an analytical model for how this cognitive bias can impact data during post-training (corroborated by empirical verifications on multiple preference datasets) and use our analysis to confirm the role of typicality bias in causing mode collapse.

%To this end, we first theoretically prove that RLHF indeed induces mode collapse by concentrating the probability mass on a narrow set of outputs (the ``mode''). %in Section~\ref{sec:proof}. 
%Guided by these insights,

% As this is a pervasive/endemic issue across all human preference data, we don’t look for solutions in training setup or modeling. Garbage in, garbage out. instead we Grounded in the theoretical insights and propose a simple yet principled prompting strategy to mitigate this issue.

 As typicality bias is pervasive across all human preference data, we look for solutions beyond the training process. 
 % Instead, 
 Grounded in our theoretical insights, we %rethink the role of prompting and 
 propose a simple but principled prompting method to bypass mode collapse. 
% Grounded in the theoretical insights, we 
% % Insights from our analysis guide us to 
% propose a simple yet principled prompting strategy to mitigate this issue, 
As shown in Figure~\ref{fig:intro_teaser},  instead of a traditional, direct prompt asking for a single instance (e.g., ``tell me a joke about coffee''), we reformulate the prompt to explicitly ask the model to \emph{verbalize} a distribution of responses with corresponding probabilities (e.g., ``generate 5 responses with their probabilities''). We call our method \textbf{\textit{\ours} \textit{(VS)}}. 
Intuitively, VS works because different prompts collapse to different modes. The modal response to a traditional instance-level prompt tends towards stereotypicality. By contrast, when prompted for a distribution, the modal response tends to approximate the distribution learned during pretraining, recovering the diversity of the underlying base model. 
% in a direct instance-level prompt, the mode is the stereotypical modal response; but with the distribution-level VS prompt, the mode can approximate the diverse distribution learned during pretraining. This is how VS recovers the diversity of the base model. 
%Importantly, our VS method operates at inference time, requires no model retraining, and works for both open-source and proprietary models as a light-weight solution to effectively improve model performance across a range of applications.
%
%Our theory explains previous empirical observations that prompting LLMs to generate a list of responses or verbalize knowledge can improve performance in various tasks like question answering \citep{tian_just_2023}, survey simulation \citep{meister_benchmarking_2024}, commonsense reasoning \citep{zhang2024improving}, and synthetic data generation \citep{wang2023self, dubois2023alpacafarm, zhu2025bareleveragingbaselanguage, si2024can}. These methods can also be seen as an implicit form of \ourslower, but they lack the theoretical grounding to fully leverage this strategy to ask for a distribution explicitly. Our work provides a theoretical understanding for mode collapse to develop a principled solution that can be generalized across a range of applications. %This work provides the first theoretical understanding of why this strategy is effective and develops a principled solution that can be generalized across various tasks.


%by utilizing the fact that the mode \emph{of a} distribution and the \emph{modal distribution} are entirely distinct concepts.
%changing the mode\wyshi{@Derek}\derek{Mike's comment}: 
% The modal response to a traditional instance-level prompt tends towards stereotypicality. By contrast, when prompted for a distribution, the modal response tends to approximate the distribution learned during pretraining, recovering the diversity of the underlying base model. 
% \wyshi{i think the issue is it's too long and complex, and does not align super well with the teaser figure}
% \derek{Ah sorry did I ctrl-z your edit? something flashed. Yeah, that's why I wanted your input on this one... it's a content edit, not a presentation edit}
% This is how VS recovers the diversity of the base model. %Importantly, our VS method operates at inference time, requires no model retraining, and works for both open-source and proprietary models as a light-weight solution to effectively improve model performance across a range of applications.
%
%Our theory explains previous empirical observations that prompting LLMs to generate a list of responses or verbalize knowledge can improve performance in various tasks like question answering \citep{tian_just_2023}, survey simulation \citep{meister_benchmarking_2024}, commonsense reasoning \citep{zhang2024improving}, and synthetic data generation \citep{wang2023self, dubois2023alpacafarm, zhu2025bareleveragingbaselanguage, si2024can}. These methods can also be seen as an implicit form of \ourslower, but they lack the theoretical grounding to fully leverage this strategy to ask for a distribution explicitly. Our work provides a theoretical understanding for mode collapse to develop a principled solution that can be generalized across a range of applications. %This work provides the first theoretical understanding of why this strategy is effective and develops a principled solution that can be generalized across various tasks.

 
% for a prompt that asks for a single response (e.g., "tell a joke about coffee"), we reformulate it to explicitly ask for a distribution of responses and  \emph{verbalize} their corresponding probabilities (e.g., ``generate 5 responses with their probabilities''), as illustrated in Figure~\ref{fig:intro_teaser}. 

% By shifting from a single-output prompt to a distribution-level prompt, we enable the base model to recover the diverse knowledge learned during pretraining, thereby overcoming the diversity loss caused by RLHF.

% Our work provides the first theoretical understanding to develop a principled solution that explicitly verbalizes the distribution, a method that can be generalized across various tasks.


% as supporting evidence, prior work also have also empirically observed that asking the LLM to generate a list of responses or verbalize their knowledge improves performance in tasks like question answering \cite{tian_just_2023}, survey simulation \cite{meister_benchmarking_2024} and synthetic data generation \cite{wang2023self, dubois2023alpacafarm, zhu2025bareleveragingbaselanguage, si2024can}. These methods can also be seen as as a form of verbalize sampling. but since they lack the theoritical understanding,  they do not fully leverage the advantage to explicitly verbalize and sample from the distribution. Here we provide the theoritical understaiding to develop a principled solution to explicitly verbalize and sample from the distribution that can be generalized across various tasks.  
% dis sample oracle principle because they still frame the task as sampling instances rather than explicitly reporting statistics


% Methods such as \textbf{Sequence} generation~\citep{meister_benchmarking_2024}, which prompts for a list of candidate responses ($ (y_1, ..., y_c) \sim p_{\text{LLM}}(y_1, ..., y_c|x_{\text{seq}}) $), and \textbf{Multi-Turn} generation, which elicit new responses across multiple turns ($ y_i \sim p_{\text{LLM}}(y|x, h_{i-1}) $), can be seen as \textit{implicit} forms of querying for the distribution. While these approaches improve diversity over single instance queries, they do not fully leverage the oracle principle because they still frame the task as sampling instances rather than explicitly reporting statistics.

% Existing papers have also empirically observed that verbalizing the distribution of  improves the distributional alignment in tasks like survey simulation tasks~\cite{meister_benchmarking_2024， zhu2025bareleveragingbaselanguage} and synthetic data generation\cite{}. But we provide a more principled solution by reframing the task as a distributional query that can be generalized across various tasks. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%diff with distributional alignment, Nicole's paper: given different choices in survey questions, ask the model to verbalize the distribution of the multiple choices in  survey questions
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% prompt the model to generate multiple candidate responses in a single pass and explicitly verbalize their corresponding probabilities,  as illustrated in Figure~\ref{fig:intro_teaser}. We call this method \textbf{\textit{\ours}} (VS). 

% n this way, it elicits the model to sample from real distributions learned
% during pre-training to bypass mode collapse. 

% VS can  bypass mode collapse because it elicits the model to sample. This forces the models to a more high-fidelity approximation of its prealignment latent distribution, to recover the suppressed diversity in base models. 


% Second, having LLMs verbalize their probability estimates has been shown to improve their calibration on question answering task~\cite{tian_just_2023} \wyshi{what's our key difference from this paper?} 

% like \emph{sequence} sampling~\citep{zhu2025bareleveragingbaselanguage} can partially alleviate mode collapse, \ourslower provides a more principled solution by reframing the task as a distributional query. This compels the models to a more high-fidelity approximation of its prealignment latent distribution, thereby recovering suppressed diversity \wyshi{this sentence is very AI}.

% While existing approaches like \emph{sequence} sampling~\citep{zhu2025bareleveragingbaselanguage} can partially alleviate mode collapse, \ourslower provides a more principled solution by reframing the task as a distributional query. This compels the models to a more high-fidelity approximation of its prealignment latent distribution, thereby recovering suppressed diversity


% \wyshi{add the theoretical proof part} explore how simple prompting strategies can effectively restore and elicit the generative diversity in aligned models, without retraining, handcrafted intervention, or task-specific tuning. 
% Our work builds on two key findings from recent literature. First, generating a sequence of candidate responses improves alignment with human preferences in survey questions~\cite{meister_benchmarking_2024}. Second, having LLMs verbalize their probability estimates has been shown to improve their calibration on question answering task~\cite{tian_just_2023} 
% % \wyshi{what's our difference with this one?}. 
% By integrating these two insights, we propose a simple inference-time prompting strategy. Given a prompt, the model will generate multiple candidate responses in a single pass and explicitly verbalize their corresponding probabilities. We call this method \textbf{\textit{\ourslower}} (VS) as illustrated in Figure~\ref{fig:intro_teaser}.

% \wyshi{also emphasize and summarize the theoretical proof, which importantly differentiates our work with other prompting studies. we provide theoretical proof that xxx. }
%\wyshi{the sentence structure is the same as the COT paper, one rule of thumb is "five consecutive words"=plagiarism}.
% \wyshi{i'd just say we propose two improvements, rather than we identify the limitations, }
% \wyshi{combine these two paragraphs, the one above and the one below, I think the theoritical proof motivated the project, not existing studies,}
% We first establish a theoretical framework that proves RLHF induces mode collapse by concentrating probability mass on a narrow set of reward-maximizing outputs. 
% While existing approaches like \emph{sequence} sampling~\citep{zhu2025bareleveragingbaselanguage} can partially alleviate mode collapse, \ourslower provides a more principled solution by reframing the task as a distributional query. This compels the models to a more high-fidelity approximation of its prealignment latent distribution, thereby recovering suppressed diversity \wyshi{this sentence is very AI}.

% \wyshi{need to refer to figure 1 more}
Building on this foundation, we conduct comprehensive experiments across creative writing (poem, joke, story generation), social dialogue simulation, synthetic data generation, and open-ended QA tasks. As shown in qualitative examples in Figure~\ref{fig:intro_teaser}, 
% \wyshi{todo: add sythetic data later}
we find that (1) on creative writing, \emph{Verbalized Sampling} significantly improves output diversity; (2) on social dialogue simulation, VS induces substantially more human-like behaviors, with some models performing on par with a dedicated fine-tuned model; (3) on synthetic data generation, VS generates more diverse synthetic data that improves downstream task performance;
% \wyshi{this seems to be an overclaim, I'd say on par with}
(4) on open-ended QA tasks with multiple valid answers, it generates a broader and more balanced response distribution. Moreover, VS supports output diversity tuning. We also confirm that VS improves performance without sacrificing the models' factual accuracy or safety. To summarize, we contribute the following: %\cmcomment{For the end of the intro, I think you definitely should have some concrete numbers showing by how much you improve diversity, with some baseline comparison, and perhaps a skyline (can there be a comparison to a model that is not post-trained?). Maybe a result in Figure 2a or table 2 could be used for this. }:
\begin{enumerate}[leftmargin=15pt, labelwidth=10pt, align=left]
    \item \textbf{Novel Cause of Mode Collapse}. We provide a new theoretical  framework to understand mode collapse, and identify and verify \textit{typicality bias} in empirical preference data as a key cause. %This offers a new data-centric lens to analyze RLHF-ed models from the angle of the properties of the preference data.  %suggesting a shift in focus from post-hoc methods to the properties of the preference data itself.
    This finding offers a new, data-driven perspective for analyzing the behavior of aligned models. %for various purposes \wyshi{get people's opinion} \cmcomment{Seems okay, but I'd delete ``for various purposes''}.
    % to analyzing the behavior of models trained via RLHF.
    
    % it's really a new way to think about RLHF tuned LLMs and how to prompt them. VS is one algorithm that we recover from the thought experiment, because our focus is diversity, but it's great empirical success suggests this may be an improved direction of thought for LLM prompting aimed at other objectives. we argue a shift in attention to analyziing the preference data in RLHF

    \item \textbf{Training-Free Solution.} Informed by our theoretical understanding, we introduce a principled prompting method, \textit{\ours}, that explicitly asks for a distribution of responses and verbalizes its corresponding probabilities, restoring LLMs' inherent generative diversity. %for diversity and simulation fidelity. %recover base models' diversity level.
    %unlock LLM potential in generative diversity.
    \item \textbf{Empirical Gains.} We perform comprehensive experiments that show VS significantly improves the diversity-quality trade-off across tasks and model families, without compromising factual accuracy and safety. For instance, in creative writing, VS boosts diversity by 1.6-2.1$\times$ over direct prompting (Figure~\ref{fig:creativity_main}), improving human evaluation scores by 25.7\% (Table~\ref{tab:human_study_diversity}), and recovering 66.8\% of the base model's diversity (Figure~\ref{fig:training_progression}). %VS further enables diversity tuning. 
    We also observe an emergent trend that more capable models benefit more from VS. These results open up possibilities in real-world tasks such as richer exploration in reinforcement learning (RL), silicon sampling, and social simulation.
    \item \textbf{Broader Implications for Alignment.} Our work shows mode collapse can be mitigated at inference time, aligned models retain significant inherent diversity, and the quality-diversity trade-off can be systematically improved through prompting alone.
    % \derek{Is it just me, or are items 1-3 too similar to the abstract? In particular, item 3.}



    
    % VS increased poem diversity to 30.4, far surpassing direct prompting (10.8) and the top baseline (17.37), and recovering much of the base model's 45.39 score.


    
    % VS achieved a diversity of 30.36, while direct prompting only has a diversity 10.79, the strongest baseline only achieve 17.37, the upperbound is the base model with 45.39 so VS can recover a large portion of the diversity from the base model.
    
     
    % \item Importantly, this work demonstrates how VS, a lightweight and inference-time solution, can unlock the inherent generative diversity of LLMs without retraining, unlocking new possibilities in real-world tasks like richer exploration in RL, silicon sampling, social simulation, and so on.
    % Importantly, VS is a lightweight, inference-time solution to improve model performance without costly retraining. This work highlights that VS can unlock aligned LLMs' inherent capacity for diversity, and showcases their potential across various applications including silicon sampling and social simulation.%This works highlights how LLMs inherently possess capabilities to generate diverse responses, and how we can their potential in various real-world applications such as silicon sampling, social simulation, and so on. % are inherently  operates at inference time, requires no model retraining, and works for both open-source and proprietary models as a light-weight solution to effectively improve model performance across a range of applications
    
    % This unlocks LLMs' potential in various real-world applications such as silicon sampling, social simulation   % \wyshi{+ without sacrificing}
\end{enumerate}
% \wyshi{verbose}. 
% \wyshi{we didn't show the safety benefits, right? i'd say without sacraficing the output quality and factualness (the commonsense task?)}
% \wyshi{mention our good performance/findings. the emergent trend, too}
% \wyshi{low priority: add a short contribution section} 
% \simoncomment{Contribution: 1. Theoretical Proof 2. Comprehensive experiments showing its performance. 3. Human Study?}

% \paragraph{Our main contributions are as follows:}
% \begin{enumerate}[leftmargin=20pt,labelsep=1em]
% \item \textbf{Enhanced creativity without quality trade-offs} (\S\ref{sec:creative_writing}): We demonstrate that \ourslower significantly improves diversity in creative writing tasks while maintaining comparable quality scores.
% \item \textbf{Improved dialogue simulation} (\S\ref{sec:dialogue_simulation_task}): \ourslower enables more realistic and diverse human behavior modeling in conversational scenarios, capturing the multifaceted nature of human responses.
% \item \textbf{Effective bias mitigation} (\S\ref{sec:bias_mitigation}): Our method reduces demographic bias in language generation by encouraging exploration of diverse perspectives and reducing mode collapse toward stereotypical responses.
% \item \textbf{Preserved truthfulness} (\S\ref{sec:commonsense}): Unlike many diversity-enhancing techniques, our approach maintains factual accuracy in commonsense reasoning tasks, showing that diversity gains do not compromise model reliability.
% \end{enumerate}
% \jiayicomment{TODO: adding results on the safety benchmark}

% \wyshi{TODO: tunable experiment, math, poem, RL}

% \wyshi{TODO: human study}

% % \wyshi{TODO: safety}
% \wyshi{TODO: ground truth distribution comparison}

% \wyshi{TODO: new metrics the open-ended QA}

% \wyshi{stretch goal: RL synthetic data generation for more complex tasks and models}
