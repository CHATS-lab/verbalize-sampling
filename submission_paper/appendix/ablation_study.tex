\section{Ablation Study}

\subsection{Ablation on \ours across RLHF stages}\label{sec:ablation_mitigation} 

\begin{wrapfigure}{r}{0.50\textwidth}
    \captionsetup{skip=2pt} 
    \vspace{-1.4em}
    \centering
    \includegraphics[width=\linewidth]{figures/creative_writing/poem/ablation/training_progression_diversity.pdf}
    \caption{
    \textbf{Diversity scores across the training stages of Tulu-70B.} The red dashed line indicates the base model's diversity level. Baseline
  prompting methods experience major degradation through SFT and DPO,
  with Direct prompting showing the most severe decline. In contrast, our methods maintain higher
  diversity scores throughout all training stages, demonstrating resilience to \emph{mode collapse}.
  \vspace{-1em}
    }
    \label{fig:training_progression}
\end{wrapfigure}
We evaluate the output diversity across different post-training stages to provide empirical evidence to show that VS can mitigate mode collapse. 
%
To do so, we employ the Tulu-3 family~\citep{lambert2025tulu3pushingfrontiers}. It contains checkpoints for SFT, RLHF and RLVR starting from Llama-3.1-70B-base models~\citep{grattafiori2024llama3herdmodels}. Figure~\ref{fig:training_progression} reveals a critical insight: while traditional prompting methods experience dramatic diversity drops as models undergo alignment training, \ours maintains a high diversity score across different training stages. Specifically, Direct prompting exhibits the most severe mode collapse, dropping from 22.5\% diversity in the base model to just 5.3\% after DPO training. In contrast, \ours shows remarkable resilience, maintaining 15\% diversity throughout, with a particularly striking +182.6\% improvement over Direct prompting at the DPO stage. This suggests that our method bypasses the mode collapse that alignment training induces in standard prompting. 

\subsection{Ablation on the Number of Candidates ($k$) in \ours}\label{sec:ablation_number_candidates}
\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/ablation/decoding_strategies/num_samples_ablation_comparison.pdf}
    \caption{\textbf{Analysis of the number of candidates ($k$) for poem generation across GPT-4.1 and Gemini-2.5-Flash.} Each plot illustrates the diversity-quality trade-off as $k$ is varied from 1 to 20. Increasing $k$ generally improves diversity but lowers quality. VS-Standard consistently provides the best trade-off, achieving a superior Pareto front.}
    \label{fig:num_candidates_ablation}
\end{figure}

We analyze the impact of the number of candidates ($k$) on the generation process. In this experiment, we vary $k$ within the set $\{1, 3, 5, 10, 15, 20\}$ for the Direct, Sequence, and VS-Standard methods, while keeping other decoding parameters fixed. The results, illustrated in Figure \ref{fig:num_candidates_ablation}, show a clear trade-off: \textbf{increasing the number of candidates consistently boosts diversity at the expense of quality across all methods and models}. However, VS-Standard (red) consistently establishes a superior Pareto front. For any given level of diversity, it maintains a higher quality score compared to both the Direct (light blue) and Sequence (blue) baselines. This indicates that our method is more effective at leveraging a larger candidate pool to find diverse yet high-quality outputs, mitigating the quality degradation typically seen when increasing $k$.

\subsection{Ablation on Decoding Strategies}\label{sec:ablation_decoding_strategies}
A key feature of Verbalized Sampling is that it is orthogonal to the decoding strategy, creating an opportunity to further enhance generation diversity. In this section, we ablate these combinations, specifically layering our method with temperature \citep{ACKLEY1985147}, top-p \citep{holtzman2020curiouscaseneuraltext}, and a recent effort called min-p sampling \citep{nguyen_turning_2025}, to systematically analyze their impact on the quality-diversity trade-off.

\paragraph{Temperature.} 
\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/ablation/decoding_strategies/poem_temperature_plot.pdf}
    \caption{\textbf{Temperature analysis for poem generation across GPT-4.1 and Gemini-2.5-Flash models.} Each plot shows the diversity-quality trade-off for three methods (Direct, Sequence, VS-Standard) at different temperature values ($t$). Higher temperatures generally increase diversity but may reduce quality. VS-Standard consistently achieves the best quality-diversity balance across both models.}
    \label{fig:temperature_ablation}
\end{figure}

We investigate the effect of sampling temperature on the diversity-quality trade-off for poem generation. We vary the sampling temperature ($t \in \{0.4, 0.6, 0.8, 1.0, 1.2, 1.4\}$) for three methods (Direct, Sequence, and VS-Standard) across two models (GPT-4.1 and Gemini-2.5-Flash). Figure \ref{fig:temperature_ablation} illustrates the diversity-quality Pareto front for each method. The results indicate that VS-Standard (red) consistently achieves a superior balance between quality and diversity across both models, pushing forward the Pareto front relative to the Direct (light blue) and Sequence (blue) baselines \citep{zhang-etal-2021-trading, padmakumar2025memorizationmappingoriginalityqualityfrontier}. Across all methods, \textbf{higher temperatures generally increase diversity at the cost of reduced quality}.

\paragraph{Top-p Sampling.}
\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/ablation/decoding_strategies/top_p_ablation_inset.pdf}
    \caption{\textbf{Top-p sampling analysis for poem generation across GPT-4.1 and Gemini-2.5-Flash.} The plots show the quality-diversity trade-off for varying $p$ values. VS-Standard demonstrates a superior performance, with an optimal balance often found at $p=0.95$. The inset provides a zoomed-in view of each method's performance curve.}
    \label{fig:top_p_ablation}
\end{figure}

Next, we explore the interaction between our method and top-p (or nucleus) sampling by varying $p \in \{0.7, 0.8, 0.9, 0.95, 1.0\}$. As shown in Figure \ref{fig:top_p_ablation}, the effect of top-p is more nuanced than that of temperature. For VS-Standard, we observe that \textbf{both quality and diversity tend to increase as $p$ is raised from 0.7 to an optimal value around 0.95}, after which quality may slightly decline. This suggests a synergistic relationship, where a moderately high $p$ value allows the model to explore a richer set of high-probability tokens that VS-Standard can effectively refine into superior outputs. Across both GPT-4.1 and Gemini-2.5-Flash, VS-Standard again carves out a more advanced Pareto front, demonstrating its robust compatibility with top-p sampling.

\paragraph{Min-p Sampling.}
\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/ablation/decoding_strategies/min_p_ablation_comparison.pdf}
    \caption{\textbf{Min-p sampling analysis for poem generation across Qwen3-235B and Llama-3.1-70B-Instruct.} The plots show the quality-diversity trade-off for varying min-p values. Increasing min-p enhances diversity while reducing quality. VS-Standard significantly outperforms the baselines, establishing a much more favorable Pareto front on both open-source models.}
    \label{fig:min_p_ablation}
\end{figure}

Finally, we evaluate VS-Standard in conjunction with min-p sampling, a recent technique that requires access to the model's logit distribution. Accordingly, we conduct this ablation on two powerful open-source models: Qwen3-235B and Llama-3.1-70B-Instruct, with $p \in \{0.0, 0.01, 0.02, 0.05, 0.1\}$. The results in Figure \ref{fig:min_p_ablation} are striking. While the general trend of \textbf{increasing min-p boosting diversity at the cost of quality} holds for all methods, VS-Standard operates on a completely different performance level. Its Pareto front is substantially superior to the baselines, maintaining exceptionally high quality even at diversity levels that cause a significant quality collapse in the Direct and Sequence methods. This confirms the effectiveness of VS-Standard on leading open-source models and its compatibility with state-of-the-art sampling techniques.


\clearpage
\subsection{Ablation on Probability Definitions in \ours}\label{sec:ablation_probability_format}
As shown in~\Cref{sec:vs}, prompting the model to verbalize the distribution of responses along with their corresponding probabilities allows \ours to overcome the mode collapse by explicitly instructing the model to sample from its original, diverse pre-training distribution. There are multiple ways to elicit these verbalized probabilities, and we explore seven variants. For example, when prompting the model to "Generate five jokes about coffee, each response with corresponding probability. The probability is defined as [probability\_definition]", the probability is defined exactly as follows:
\begin{itemize}
    \item \textbf{Implicit probability}: ``how likely this response would be (from 0.0 to 1.0)'' (mentioned the full distribution implicitly); 
    % \wyshi{@jiayi can you just put an example prompt here?}
    \item \textbf{Explicit probability}: ``the estimated probability from 0.0 to 1.0 of this response given the input prompt (relative to the full distribution)'' (mentioned the full distribution explicitly);
    \item \textbf{Relative probability}: ``the probability between 0.0 and 1.0, reflecting the relative likelihood of this response given the input.''; 
    \item \textbf{Percentage probability}: ``the probability of this response relative to the full distribution, expressed as a percentage from 0\% to 100\%'';
    \item \textbf{Confidence}: ``the normalized likelihood score between 0.0 and 1.0 that indicates how representative or typical this response is compared to the full distribution''; 
    \item \textbf{Perplexity}: ``the exponentiated average negative log likelihood of the response tokens, where lower values indicate higher model certainty in predicting each token''; 
    \item \textbf{Negative Log-likelihood (NLL)}: ``the sum of the negative log probabilities of each token in the response given the input prompt, with smaller values reflecting higher model confidence'.
\end{itemize}

The VS prompt can be found in~\Cref{appendix:experiment_prompt}, where the definition in the probability field can be replaced with the exact definition provided above. To investigate which form of verbalized probability best reflects the true pre-training distribution and leads to improved task performance, we conduct an ablation study on two settings: poem continuation (a creative writing task) and open-ended QA. We selected these tasks because poem continuation has an effectively unlimited answer space, whereas open-ended QA has a more constrained answer space. This allows us to examine how different forms of verbalized probability affect performance across varying output spaces.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/creative_writing/verbalized_sampling_prob_format.png}
    \caption{\textbf{Ablation of probability formats for \ours on Creativity Task.} We evaluate VS-Standard (\textcolor{ProcessBlue}{blue}) and VS-Multi (\textcolor{Salmon}{red}) on two models across three metrics: \textbf{(a, c)} Diversity ($\uparrow$) and \textbf{(b, d)} Quality ($\uparrow$). Subplots \textbf{a–b} report results on GPT-4.1, while \textbf{c-d} show results on Gemini 2.5 Flash. Prompt formats include Implicit, Explicit, Relative, Percentage, Confidence, NLL, and Perplexity. 
    % \wyshi{the font of the title is not the same as others}
    }
    \label{fig:ablation_bias_probability_prompts}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{figures//ablation/bias_prompts_ablation_line_chart.pdf}
    \caption{\textbf{Ablation of probability formats for \ours on Open-ended QA Task.} We evaluate VS-Standard (\textcolor{ProcessBlue}{blue}) and VS-Multi (\textcolor{Salmon}{red}) on two models across three metrics: \textbf{(a, d)} KL Divergence (↓), \textbf{(b, e)} Coverage-N (↑), and \textbf{(c, f)} Precision (↑). Subplots \textbf{a–c} report results on GPT-4.1, while \textbf{d–f} show results on Gemini 2.5 Flash.
    }
    \label{fig:ablation_bias_probability_prompts}
\end{figure}

\paragraph{Results and Analysis.} 
As shown in \Cref{fig:ablation_bias_probability_prompts} (a–d), both VS-Standard and VS-Multi outperform the baselines in terms of diversity on GPT-4.1 and Gemini-2.5-Flash. Across probability formats, we observe no significant overall advantage of one format over another. For both models, VS-Standard tends to perform best with \textit{Explicit}, while VS-Multi generally benefits more from \textit{Confidence}. In terms of quality, differences across formats remain small, with VS-Multi showing a slight overall advantage over VS-Standard.  

For open-ended QA (\Cref{fig:ablation_bias_probability_prompts} a–f), VS-Standard (blue) shows limited variance across probability formats, with \textit{Explicit} performing slightly better on KL Divergence and Coverage-N. VS-Multi (red), in contrast, benefits more consistently from \textit{Explicit} and \textit{Confidence}, though other formats are less stable. Precision under VS-Standard remains stable across formats, while VS-Multi exhibits greater sensitivity, particularly on Gemini-2.5-Flash. 

Overall, we find that VS-Standard tends to benefit most from the \textit{Explicit} format, while VS-Multi often prefers \textit{Confidence}. However, these preferences vary by model, and no single format provides a universally significant improvement. This suggests that although explicit grounding of likelihood values is often beneficial, the optimal probability format should be adapted to the model and task.


\newpage
\subsection{Ablation on Probability Manipulation in VS on Creative Writing} \label{sec:ablation_diversity_tuning_creativity}

One key advantage of \ours over baseline methods is the ability to control output diversity through prompting alone, a capability we term \textit{diversity tuning}. Unlike traditional approaches that require retraining or architectural modifications to adjust diversity, \ours enables fine-grained control by manipulating the probability thresholds in the verbalization prompt (e.g., ``sample from tail distribution, where each response should be $< p\%$'').

\paragraph{Experimental Setup.}
We conduct systematic experiments across different probability tuning parameters $p \in \{1.0, 0.9, 0.5, 0.2, 0.05, 0.005, 0.001\}$, where $p = 1.0$ indicates no diversity tuning is applied (standard \ours behavior). We prompt models to ``sample from tail distribution, where each word should be $< p\%$'' to manipulate the probability thresholds in the verbalization process. We evaluate \ours on joke, poem, and story generation tasks using GPT-4.1 and Gemini 2.5 Flash.

\paragraph{Results and Analysis.}
Figure~\ref{fig:diversity_tuning_joke} and Figure~\ref{fig:diversity_tuning_poem} demonstrate the effectiveness of probability-based diversity control across both tasks and models. \ours exhibits smooth, controllable diversity curves as the probability threshold varies, with lower probability thresholds generally producing higher diversity outputs. Across all probability settings, \ours significantly outperforms the Direct and Sequence baselines, with the performance gap being particularly pronounced in joke generation, requiring a broken y-axis visualization due to substantial diversity improvements.

The results reveal consistent diversity tuning patterns across different model architectures, confirming the robustness of the probability manipulation approach. Task-specific optimal ranges emerge, with joke generation benefiting from moderate to low probability thresholds (0.05-0.5), while poem generation shows more nuanced patterns across the parameter space. This ablation study confirms that probability manipulation in \ours provides a practical mechanism for diversity control, offering users fine-grained control over output creativity through prompting alone.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/appendix/diversity_tuning/joke_diversity_tuning_comparison.pdf}
    \caption{
    \textbf{Diversity tuning results for joke generation.}
    Comparison of diversity scores across probability
  tuning parameters for GPT-4.1 (left) and Gemini 2.5 Flash
  (right). The x-axis shows probability thresholds in descending
  order from 1.0 to 0.001. VS-Standard and VS-Multi consistently
  outperform Direct and Sequence baselines across all parameter
  settings. The broken y-axis highlights the substantial
  performance gap between \ours methods and baselines. Both VS
  variants demonstrate smooth, controllable diversity curves,
  with VS-Multi achieving slightly higher peak diversity values.
    }
    \label{fig:diversity_tuning_joke}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/appendix/diversity_tuning/poem_diversity_tuning_comparison.pdf}
    \caption{
    \textbf{Diversity tuning results for poem generation.}
    Comparison of diversity scores across probability
  tuning parameters for GPT-4.1 (left) and Gemini 2.5 Flash
  (right). The continuous y-axis shows the full range of
  diversity values. VS-Standard and VS-Multi maintain consistent
  performance advantages over baselines while exhibiting
  complementary tuning behaviors. The results demonstrate that
  probability manipulation provides effective diversity control
  across different model architectures, with optimal parameter
  ranges varying based on the specific creative task.
    }
    \label{fig:diversity_tuning_poem}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/appendix/diversity_tuning/book_diversity_tuning_comparison.pdf}
    \caption{
    \textbf{Diversity tuning results for story generation.}
    Comparison of diversity scores across probability
  tuning parameters for GPT-4.1 (left) and Gemini 2.5 Flash
  (right). The continuous y-axis shows the full range of
  diversity values. VS-Standard and VS-Multi maintain consistent
  performance advantages over baselines while exhibiting
  complementary tuning behaviors. The results demonstrate that
  probability manipulation provides effective diversity control
  across different model architectures, with optimal parameter
  ranges varying based on the specific creative task.
    }
    \label{fig:diversity_tuning_story}
\end{figure}


\newpage
\subsection{Ablation on Probability Manipulation in VS on Open-Ended QA} \label{sec:diversity_tuning_open_ended_qa}
Following the probability manipulation experiments on the creativity tasks in \Cref{sec:ablation_diversity_tuning_creativity}, we conducted the same experiment on the Open-Ended QA task. Unlike creativity tasks, this task has a more constrained answer space, where probabilities can be more clearly interpreted. 

\paragraph{Experimental Setup.} We conduct systematic experiments across different probability tuning parameters $p \in \{1.0, 0.9, 0.5, 0.1, 0.05, 0.01\}$, where $p = 1.0$ indicates no diversity tuning is applied (standard \ours behavior). 
We used the same prompting strategy, explicitly instructing the model to sample from the distribution such that the probability of each response $< p\%$, thereby controlling the probability thresholds in the verbalization process. 
We excluded thresholds below $0.01$, as such extremely tailed distributions often led the model to return empty outputs.
Experiments were conducted on the full Open-Ended QA set with $N=40$ and $k=20$, using GPT-4.1 and Gemini-2.5-Flash.

\paragraph{Results and Analysis.} As shown in \Cref{fig:diversity_tuning_coverage_n}, VS-Standard and VS-Multi consistently outperform the sequence baseline. For GPT-4.1, Coverage-N improves as $p$ decreases, peaking near $p=0.1$ before slightly dropping at $p=0.01$. A similar trend is observed for Gemini-2.5-Flash, where coverage improves notably at moderate probability thresholds. These results suggest that moderate probability constraints encourage the model to explore a broader range of plausible answers, thereby enhancing diversity. However, extremely low thresholds ($p \leq 0.01$) lead to diminishing returns, as the distribution becomes overly tailed and unstable.

\Cref{fig:diversity_tuning_kl_divergence} shows a general decreasing trend in KL Divergence as $p$ decreases, reflecting closer alignment with the uniform distribution. Both GPT-4.1 and Gemini-2.5-Flash benefit from tuning, though GPT-4.1 spikes at $p=0.01$, indicating instability when sampling from very low-probability regions. Across models, VS-Standard and VS-Multi consistently achieve lower divergence than the sequence baseline.

Together, these findings indicate that probability tuning enhances response diversity in Open-Ended QA, with the strongest gains observed at moderate thresholds (e.g., $p \leq 0.1$). While VS-Standard already provides consistent improvements, VS-Multi offers additional flexibility in exploring the answer space, though very small probability cutoffs can introduce instability.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/ablation/bias_metrics_tuning_coverage_n.pdf}
    \caption{\textbf{Diversity tuning results for Open-Ended QA on Coverage-N.} Results are shown for GPT-4.1 (left) and Gemini-2.5-Flash (right) across probability tuning parameters. Coverage-N measures the proportion of ground truth covered in the response distribution (higher is better). Both VS-Standard and VS-Multi consistently outperform the sequence baseline, with coverage increasing as probability decreases until $\leq 0.1$, where the distribution becomes heavily tailed.
    }
    \label{fig:diversity_tuning_coverage_n}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/ablation/bias_metrics_tuning_kl_divergence.pdf}
    \caption{\textbf{Diversity tuning results for Open-Ended QA on KL Divergence.} Results are shown for GPT-4.1 (left) and Gemini-2.5-Flash (right) across probability tuning parameters. VS-Standard and VS-Multi achieve consistently lower divergence than the sequence baseline. The overall trend shows decreasing KL Divergence as probability decreases, indicating closer alignment with uniform distribution.
    }
    \label{fig:diversity_tuning_kl_divergence}
\end{figure}