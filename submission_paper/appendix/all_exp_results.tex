\section{Detailed Experimental Results}\label{appendix:exp_results}

\input{appendix/app_exp_creativity_task}

\clearpage
\input{appendix/app_exp_dialogue_simulation_task}

\newpage
\input{appendix/app_exp_open_ended_qa}

\newpage
\input{appendix/app_exp_commonsense}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Random Number Generation %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsection{Random Number Generation}\label{sec:random_number_generation}
\begin{wrapfigure}{r}{0.5\textwidth}
    \captionsetup{skip=2pt}
    \centering
    \captionof{table}{Average KL divergence across models for each method in the dice roll experiment. The best result is in \sethlcolor{LightBlue}\hl{\textbf{blue}}; the second-best is \sethlcolor{LightGreen}\underline{\hl{green}}.}
    \label{tab:rng_table}
    \begin{minipage}{\linewidth}
        \centering
        \begin{tabular}{lc}
            \toprule
            Method & KL Divergence $\downarrow$ \\
            \midrule
            Direct & 0.926 \\
            CoT & 1.163 \\
            Multi-turn & 0.119 \\
            Sequence & 0.058 \\
            VS-Standard & \bestcell{0.027} \\
            VS-CoT & 0.038 \\
            VS-Multi & \secondcell{0.029} \\
            \bottomrule
        \end{tabular}
    \end{minipage}

    \vspace{4pt}

    % Figure with its own caption and label
    \begin{minipage}{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/qualitative_tasks/rng_distribution_comparison.pdf}
        \captionof{figure}{Dice roll distributions from direct, sequence, and \ourslower prompting with Gemini-2.5-Pro. The red dashed line marks the expected uniform distribution: VS aligns most closely, sequence follows, while direct prompting collapses to a single mode (e.g., 4). 
        \vspace{-1em}
        % \wyshi{this figure needs to be refined}
        % \wyshi{why somietimes there is grid in the background, this one doesnt}
        }
        \label{fig:qualitative_rng_results}
    \end{minipage}
\end{wrapfigure}


We also wondered if \ours (VS) can achieve randomness, which is critical for tasks that require unpredictability in random processes, for example, paper-scissor-stone~\citep{west_base_2025}.
To evaluate this, we assess whether VS enables LLMs to better approximate random behavior in a simple setting: rolling a fair 6-sided dice. For each method, we prompt the model to simulate a dice roll, sampling $N=600$ responses and $k=5$ responses for each LLM call. We then calculate the KL divergence between the empirical distribution of the generated numbers and the true uniform distribution. This allows us to quantitatively assess how well each method captures true randomness. 

\Cref{tab:rng_table} presents the average KL divergence across models for the dice roll experiment using different prompting methods. Complementarily, Figure~\ref{fig:qualitative_rng_results} offers a more closer look of the dice roll distributions under direct, sequence, and VS prompting with Gemini-2.5-Pro.
Direct prompting produces a highly skewed distribution, often collapsing to a single outcome (e.g., rolling a 4), which is reflected in a high KL divergence ($0.926$). Direct with chain-of-thought performs even worse ($1.163$), while multi-turn improves but remains imperfect ($0.119$). In contrast, both sequence prompting ($0.058$) and our verbalized sampling variants achieve distributions that closely approximate the expected uniform distribution. Among them, VS-Standard achieves the lowest KL divergence, followed closely by VS-Multi and VS-CoT. These results confirm that verbalized sampling consistently improves randomness modeling, aligning closely with the theoretical uniform distribution and substantially outperforming direct and other baseline prompting strategies.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Synthetic Data Generation %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsection{Synthetic Data Generation}\label{appendix:synthetic_data}

\subsubsection{Positive Synthetic Data Generation}
\label{appendix:positive data}

\paragraph{Synthetic Data Generation Setup.} To ensure comparable results with related work~\citep{liu2025understandingr1zeroliketrainingcritical}, we use the same temperature of $0.6$ and top-p of $0.95$ for the answer generation.

\paragraph{Finetuning on Synthetic Data.} The training is done with 5 epochs and a learning rate of $5e-6$. 


% Recent research has shown that the diversity of synthetic data plays an important role in improving downstream model performance. For example,~\citet{chen_diversity_2024} demonstrate that higher synthetic data diversity leads to better pre-training and fine-tuning outcomes, and propose metrics such as cluster-based diversity to capture this effect. Similarly, the BARE framework~\citep{zhu2025bareleveragingbaselanguage} highlights the value of generating synthetic data that is both diverse and indistinguishable from real data, introducing the indistinguishable rate as a quality measure. Motivated by these findings, we introduce the \textbf{positive synthetic data generation task} to further evaluate the effectiveness of \ours (VS).

% In our experiments, we closely follow the setup of BARE~\citep{zhu2025bareleveragingbaselanguage}. However, instead of providing three seed in-context examples and prompting the model to generate question–answer pairs, we directly prompt the model to generate synthetic questions for GSM8K~\citep{cobbe2021trainingverifierssolvemath}, and then GPT-4.1~\citep{openai2025gpt41} is used to generate the corresponding answers. For each method, we sample a total of $N=1000$ responses, with each LLM call producing $k=5$ responses.
% To evaluate \textit{diversity}, we compute \textbf{pairwise semantic diversity} using OpenAI's \texttt{text-embedding-3-small} embeddings~\citep{openai2024embedding} to calculate the average pairwise cosine similarity, as well as average of \textbf{Distinct-1/2/3} for surface-level variation. 
% For \textit{quality}, we report the \textbf{indistinguishability rate} (IR) following ~\citep{zhu2025bareleveragingbaselanguage}, which defines quality as the proportion of cases where a strong LLM judge cannot reliably differentiate synthetic samples from the real ones.

% As shown in \Cref{fig:synthetic_positive_combined_results}, VS achieves the strongest overall balance between diversity and quality. While sequence prompting sometimes yields slightly higher diversity (e.g., the semantic diversity and Distinct-N on GSM8K dataset) and multi-turn prompting achieves higher indistinguishable rates, these gains come at the cost of trade-offs in the other dimension. In contrast, \ours consistently delivers competitive diversity scores close to the best-performing methods while simultaneously maintaining high indistinguishable rates. \Cref{fig:synthetic_positive_combined_results} (d) provides a closer examination of semantic diversity under direct and VS-Standard prompting. The results show that while VS-Standard achieves slightly higher embedding similarities than sequence, it still produces lower similarities than direct prompting, confirming its ability to generate more diverse samples.

% \begin{figure}[!ht] % 't' means top, use 'b' for bottom
%     \centering
%     \includegraphics[width=\textwidth]{figures/qualitative_tasks/synthetic_data_combined_metrics.pdf}
%     \caption{Average diversity and quality results on the \textbf{positive synthetic data generation} task with GPT-4.1.
% \textbf{(a)} Indistinguishability rate (IR), where higher values indicate better quality; Direct achieves the highest score.
% \textbf{b–c} Diversity metrics: \textbf{(b)} proportion of unique n-grams (Distinct-N) and \textbf{(c)} pairwise semantic diversity.
% \textbf{(d)} Distribution of pairwise cosine similarity, providing a closer view of semantic diversity, where lower similarity corresponds to greater diversity.
%     % \wyshi{can you just combine this with fig11, similar to fig13. this will reduce cognitive load to think about what each figure means. also, small text on the bars. Can you two share the same configs for the figure, like font size for the title, for the caption, for the numbers on the bar. for the y labels.}
%     \vspace{-1em}
%     }
%     \label{fig:synthetic_positive_combined_results}
% \end{figure}

% \paragraph{Fine-tuning.}

% \begin{table*}[!ht]
% \centering
% \caption{\textbf{Full Supervised Fine-Tuning (SFT) accuracy on the entire GSM8K test set.} We fine-tune the Llama-3.2-1B-Instruct model on 1k positive examples for each method. The values in parentheses show the absolute improvement over the ``Golden Train'' baseline. The best result is in \textbf{bold}. \textsuperscript{†}As reported by the BARE paper~\citep{zhu2025bareleveragingbaselanguage}, trained with LoRA only.}
% \label{tab:positive_data_training}
% \begin{tabular}{lc}
% \toprule
% \textbf{Method} & \textbf{Accuracy (\%)} \\
% \midrule
% \multicolumn{2}{l}{\textit{References}} \\
% \quad Llama-3.2-1B-Instruct & 21.61 \\
% \quad GSM8k (BARE~\citep{zhu2025bareleveragingbaselanguage}) & 29.8\textsuperscript{†} \\
% \quad GSM8k (Original Training dataset) & 34.12 \\
% \quad GSM8k (Golden Prompts + Response regenerated by GPT-4.1) & 46.50 \\
% \midrule
% \multicolumn{2}{l}{\textit{Our Methods (trained on 1k generated data by GPT-4.1)}} \\
% \quad Direct & 40.07 ($\uparrow$ 5.9) \\
% \quad CoT & 45.19 ($\uparrow$ 11.1) \\
% \quad Sequence & 44.88 ($\uparrow$ 10.8) \\
% \quad Multi-Turn & 44.73 ($\uparrow$ 10.6) \\
% \midrule
% \quad VS Standard & 46.63 ($\uparrow$ 12.5) \\
% \quad VS CoT & \textbf{47.92} ($\uparrow$ 13.8) \\
% \quad VS Multi & 47.31 ($\uparrow$ 13.2) \\
% \bottomrule
% \end{tabular}
% \\
% \vspace{0.5em} % Adds a little space
% \end{table*}

% We then used the data for fine-tuning Llama-3.2-1B-Instruct models, following the setup from BARE~\citep{zhu2025bareleveragingbaselanguage}. The training is based on the open-source RL2 framework~\citep{Tan2025RL2}. We trained on 1k data generated by each method and evaluated them on the GSM8K test set. The results are shown in \Cref{tab:positive_data_training}.

% The results demonstrate the clear superiority of our data generation methods. All approaches significantly outperform the Llama-3.2-1b instruct baseline (21.61\%), the BARE model (29.8\%), and the model trained on 1k human-annotated golden data (34.12\%). The verification and selection (VS) strategies proved most effective, with {VS Standard}, {VS Multi}, and {VS CoT} achieving accuracies of 46.63\%, 47.31\%, and \textbf{47.92\%}, respectively. The top score from {VS CoT} marks a 13.8 point improvement over the golden data baseline, highlighting the high quality of the synthetically generated training examples.

\begin{table}[ht]
\centering
\small
\robustify\bfseries

\caption{Performance of the \textbf{Qwen2.5-7B} model. Results compare fine-tuning on data generated by GPT-4.1 vs. Gemini-2.5-Flash.}
\label{tab:results_qwen_7b}
\setlength{\tabcolsep}{6pt} % Adjust spacing between columns
\renewcommand{\arraystretch}{1.2} % Adjust spacing between rows
\begin{tabular}{l S[table-format=2.1]S[table-format=2.1] S[table-format=2.1]S[table-format=2.1] S[table-format=2.1]S[table-format=2.1] S[table-format=2.1] S[table-format=2.1]}
\toprule
& \multicolumn{4}{c}{\textbf{GPT-4.1}} & \multicolumn{4}{c}{\textbf{Gemini-2.5-Flash}} \\
\cmidrule(lr){2-5} \cmidrule(lr){6-9}
\textbf{Method} & {\small Math500} & {\small Olympiad} & {\small Minerva} & {\textbf{Avg.}} & {\small Math500} & {\small Olympiad} & {\small Minerva} & {\textbf{Avg.}} \\
\midrule
\quad Baseline Model  & 44.4  & 19.7  & 17.6  & 27.2 & 44.4  & 19.7  & 17.6  & 27.2 \\
\midrule
\quad Direct          & 40.6  & 21.2  & 16.4  & 26.1 & 40.2  & 21.0  & 13.6  & 24.9 \\
\quad CoT             & 48.2  & 24.9  & 17.3  & 30.1 & 44.8  & 19.3  & 18.7  & 27.6 \\
\quad Sequence        & 52.0  & 22.7  & 16.9  & 30.5 & 47.2  & 23.9  & 13.6  & 28.2 \\
\quad Multi-Turn      & 49.2  & 21.8  & 18.6  & 29.9 & 44.4  & 21.5  & 15.4  & 27.1 \\
\midrule
\quad VS-Standard     & 52.8  & 26.3  & 19.0  & 32.7 & 49.8  & 22.9  & 13.2  & 28.6 \\
\quad VS-CoT          & 53.6  & 27.0  & 19.6  & 33.4 & 50.6  & 21.5  & 16.2  & 29.4 \\
\quad VS-Multi        & \bfseries{55.4} & \bfseries{27.6} & \bfseries{21.3} & \bfseries{34.8} & \bfseries{51.0} & \bfseries{24.9} & \bfseries{19.1} & \bfseries{31.7} \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[ht]
\centering
\small
\caption{Performance of the \textbf{Qwen3-1.7B-Base} model. Results compare fine-tuning on data generated by GPT-4.1 vs. Gemini-2.5-Flash.}
\label{tab:results_qwen_1.7b}
\setlength{\tabcolsep}{6pt} % Adjust spacing between columns
\renewcommand{\arraystretch}{1.2} % Adjust spacing between rows
\begin{tabular}{l S[table-format=2.1]S[table-format=2.1] S[table-format=2.1]S[table-format=2.1] S[table-format=2.1]S[table-format=2.1] S[table-format=2.1] S[table-format=2.1]}
\toprule
& \multicolumn{4}{c}{\textbf{GPT-4.1}} & \multicolumn{4}{c}{\textbf{Gemini-2.5-Flash}} \\
\cmidrule(lr){2-5} \cmidrule(lr){6-9}
\textbf{Method} & {\small Math500} & {\small Olympiad} & {\small Minerva} & {\textbf{Avg.}} & {\small Math500} & {\small Olympiad} & {\small Minerva} & {\textbf{Avg.}} \\
\midrule
\quad Baseline Model  & 53.2 & 20.2 & 18.2 & 30.5 & 53.2 & 20.2 & 18.2 & 30.5 \\
\midrule
\quad Direct          & 54.8 & 20.3 & 19.1 & 31.4 & 51.7 & 20.0 & 16.8 & 29.5 \\
\quad CoT             & 55.6 & 21.3 & 20.6 & 32.5 & 54.5 & 23.1 & 18.6 & 32.1 \\
\quad Sequence        & 54.4 & 19.0 & 19.7 & 31.0 & 54.2 & 22.7 & 18.2 & 31.7 \\
\quad Multi-Turn      & 56.4 & 21.0 & 18.4 & 31.9 & 55.3 & 23.3 & 17.9 & 32.2 \\
\midrule
\quad VS-Standard     & 54.2 & 22.7 & \bfseries{23.9} & 33.6 & 54.8 & 24.9 & 20.2 & 33.3 \\
\quad VS-CoT          & 56.0 & 23.5 & 21.6 & 33.7 & \bfseries{57.4} & \bfseries{28.3} & \bfseries{21.6} & \bfseries{35.8} \\
\quad VS-Multi        & \bfseries{56.6} & \bfseries{25.4} & 22.6 & \bfseries{34.9} & 56.3 & 27.2 & 20.9 & 34.8 \\
\bottomrule
\end{tabular}
\end{table}


\begin{table}[ht]
\centering
\small
\caption{Performance of the \textbf{Qwen3-4B-Base} model. Results compare fine-tuning on data generated by GPT-4.1 vs. Gemini-2.5-Flash.}
\label{tab:results_qwen_4b}
\setlength{\tabcolsep}{6pt} % Adjust spacing between columns
\renewcommand{\arraystretch}{1.2} % Adjust spacing between rows
\begin{tabular}{l S[table-format=2.1]S[table-format=2.1] S[table-format=2.1]S[table-format=2.1] S[table-format=2.1]S[table-format=2.1] S[table-format=2.1] S[table-format=2.1]}
\toprule
& \multicolumn{4}{c}{\textbf{GPT-4.1}} & \multicolumn{4}{c}{\textbf{Gemini-2.5-Flash}} \\
\cmidrule(lr){2-5} \cmidrule(lr){6-9}
\textbf{Method} & {\small Math500} & {\small Olympiad} & {\small Minerva} & {\textbf{Avg.}} & {\small Math500} & {\small Olympiad} & {\small Minerva} & {\textbf{Avg.}} \\
\midrule
\quad Baseline Model  & 65.4 & 33.8 & 22.8 & 40.7 & 65.4 & 33.8 & 22.8 & 40.7 \\
\midrule
\quad Direct          & 55.6 & 29.8 & 18.0 & 34.5 & 60.4 & 29.6 & 20.7 & 36.9 \\
\quad CoT             & 68.2 & 29.1 & 21.0 & 39.4 & 61.4 & 33.6 & 26.5 & 40.5 \\
\quad Sequence        & 67.6 & 35.2 & 23.6 & 42.1 & 65.6 & 34.6 & \textbf{27.3} & 42.5 \\
\quad Multi-Turn      & 64.4 & 31.9 & 27.6 & 41.3 & 54.5 & 31.5 & 25.4 & 37.1 \\
\midrule
\quad VS-Standard     & 68.0 & \bfseries{40.2} & 28.4 & 45.5 & 66.2 & 35.2 & 27.1 & 42.8 \\
\quad VS-CoT          & \bfseries{69.4} & 38.6 & \bfseries{29.7} & \bfseries{45.9} & 67.0 & \bfseries{36.7} & 26.6 & 43.4 \\
\quad VS-Multi        & 68.0 & 38.6 & 28.4 & 45.0 & \bfseries{68.0} & 35.8 & 26.9 & \bfseries{43.6} \\
\bottomrule
\end{tabular}
\end{table}


% \newpage
\subsubsection{Negative Synthetic Data Generation}
\label{sec:negative synthetic data}
Recent work emphasizes that, beyond generating diverse and realistic synthetic data, constructing challenging negative examples is also crucial for improving model robustness. For instance, \citet{Bartolo_2021} show that augmenting training with synthetically generated adversarial data enhances robustness in question answering, while \citet{setlur2024rlincorrectsyntheticdata} shows that combining supervised fine-tuning on correct solutions with RL on incorrect synthetic steps improves LLM math reasoning efficiency up to eightfold by using per-step credit assignment to reduce spurious correlations.
Motivated by these findings, we introduce a negative synthetic data generation task to evaluate whether our method can generate diverse, high-quality negative examples that are both convincing and pedagogically useful for training. 

We first test our method on generating convincing and reasonable but incorrect solutions to the GSM8K dataset~\citep{cobbe2021trainingverifierssolvemath}. We randomly select 50 questions from the dataset. For each questions, we sample $N=10$ responses and $k=5$ responses for each LLM call using GPT-4.1. To assess \textit{diversity}, we compute the \textbf{pairwise cosine similarity} of OpenAI’s \texttt{text-embedding-3-small} embeddings~\citep{openai2024embedding} within each prompt group. For \textit{quality} evaluation, we use two metrics: the \textbf{incorrect answer rate}, which measures the proportion of responses that successfully follow the instruction to generate reasonable but incorrect solutions, and the \textbf{incorrect answer coverage}, which measures the proportion of responses that different from the previous incorrect solution.

\begin{figure*}[h]
  \centering
  \includegraphics[width=0.9\textwidth]{figures/qualitative_tasks/synthetic_data_negative_gpt-4.1_diversity_quality_histogram.pdf}
  \caption{Average diversity and quality results with GPT-4.1 on the \textbf{negative synthetic data generation} task.
  \textbf{(a)} and \textbf{(b)} shows incorrect answer rate and coverage (both are the higher the better), with VS-Standard outperforming all baselines and VS-CoT achieving the best results.
  \textbf{(c)} and \textbf{(d)} shows average semantic diversity across prompting methods and semantic similarity for synthetic negative solutions across 50 GSM8K questions. Lower similarity indicates greater semantic diversity.
  }
  \label{fig:synthetic_negative_combined_results}
\end{figure*}

\Cref{fig:synthetic_negative_combined_results} shows the overall performance of the negative synthetic data generation task using GPT-4.1 across all prompting methods.
For data quality in Figure~\ref{fig:synthetic_negative_combined_results} (a) and (b), VS-Standard significantly improves both the incorrect answer rate and coverage compared to sequence, multi-turn, and other baseline promptings, demonstrating stronger ability to generate varied wrong answers. VS-CoT achieves the best overall results, with the highest incorrect answer rate (0.892) and coverage (0.572). In contrast, direct prompting often fails to follow the instruction, producing correct answers 64\% of the time, and when it does generate incorrect ones, they mostly collapse into the same solution.
For diversity in Figure~\ref{fig:synthetic_negative_combined_results} (c), VS-Standard again outperforms sequence and multi-turn, producing a broader range of distinct incorrect solutions. Figure~\ref{fig:synthetic_negative_combined_results} (d) offers a closer look: VS-Standard exhibits lower embedding cosine similarities than direct prompting, with the distribution shifted further to the left. It also yields slightly lower similarities than sequence prompting, indicating greater semantic diversity. VS-CoT further pushes this trend, achieving the highest semantic diversity while maintaining strong correctness metrics.

\paragraph{Offline-RL Results.}
\begin{table}[h]
\centering
\caption{\textbf{Accuracy on GSM8K after offline RL training.} Each experiment mixes 1k golden positive data with 1k synthetic negative data generated by the specified method. The best result is in \textbf{bold}.}
\label{tab:negative_data_training}
\begin{tabular}{lc}
\toprule
\textbf{Training Data} & \textbf{Accuracy (\%)} \\
\midrule
GSM8k (1k positive only) & 34.12 \\
\midrule
\multicolumn{2}{l}{\textit{1k positive + 1k negative from...}} \\
\quad Direct & 34.44 \\
\quad CoT & 34.67 \\
\quad Sequence & 33.42 \\
\quad Multi-Turn & 34.34 \\
\quad VS-Standard & 36.63 \\
\quad VS-CoT & \textbf{36.81} \\
\quad VS-Multi & 35.25 \\
\bottomrule
\end{tabular}
\end{table}
We perform offline RL by mixing 1k golden positive examples with 1k synthetic negative examples (randomly select 200 questions from GSM8K; for each questions, we sample $N=5$ responses and $k=5$ responses for each LLM call using GPT-4.1). Golden data is assigned a reward label of $+1$ and negative data a label of $-1$. We then optimize the policy $\pi_{\theta}$ using the following sigmoid loss function:
$$
\mathcal{L}(\theta) = -\mathbb{E}_{(x, y, L) \sim \mathcal{D}} \left[ \log \sigma \left( L \cdot \log \pi_{\theta}(y|x) \right) \right]
$$
where $L \in \{+1, -1\}$ is the label for a prompt-completion pair $(x, y)$, and $\sigma$ is the sigmoid function. The training uses the RL2 framework~\citep{Tan2025RL2}.

The results are presented in \Cref{tab:negative_data_training}. The baseline model, trained only on 1k positive golden examples, achieves an accuracy of 34.12\%. By incorporating 1k synthetic negative examples, most methods show a modest improvement. \ours again prove to be the most effective. Specifically, mixing negative data from {VS-Standard} and {VS-CoT} boosts the accuracy to 36.63\% and a new high of \textbf{36.81\%}, respectively. This demonstrates that learning to distinguish between correct and synthetically generated incorrect reasoning paths can further refine the model's capabilities, though the gains are smaller than those from positive-only SFT. Interestingly, negative data from the {Sequence} method slightly degraded performance, suggesting the quality of negative examples is crucial.

While these results demonstrate the benefit of with offline-RL, we believe our methods hold even greater promise in an online RL setting. Recent studies have emphasized the importance of diversity in rollout for RL performance~\citep{cui2025entropymechanismreinforcementlearning, wang20258020rulehighentropyminority}. We believe \ourslower provides the ideal solution to enhance the diversity when sampling and mitigate mode collapse. This would allow the policy to explore and learn from a richer set of rollouts, potentially leading to more significant and robust improvements.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Safety Evaluation %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsection{Safety Evaluation}
\label{sec:safety}

A primary concern with our approach is that while it effectively reduces mode collapse, it might inadvertently bypass the model's safety alignment, potentially leading to harmful content or functioning as a jailbreak method. To investigate this, we evaluated our approach on 353 harmful prompts from the StrongReject benchmark, using their official safety judge for assessment~\citep{souly2024strongreject}. Our experiments included six models: GPT-4.1, Gemini 2.5 Flash, Claude 3.7 Sonnet, Claude 4 Sonnet, Llama 3.1-70B, and Qwen3-235B. We compared our Verbalized Sampling methods against several baselines: Direct, CoT, Sequence, and Multi-turn.

\begin{table}[htbp]
\centering
\caption{\textbf{Safety Evaluation on \ours.} Summary statistics for the refusal rate on harmful prompts. The $\Delta$ column shows the change relative to the Direct baseline.}
\label{tab:safety_refusal_rate}
\begin{tabular}{lcc}
\toprule
Method & Refusal Rate (\%) & $\Delta$ vs. Direct (\%) \\
\midrule
Direct & $98.22 \pm 1.22$ & -- \\
CoT & $98.58 \pm 0.84$ & +0.36 \\
Sequence & $97.20 \pm 1.41$ & -1.02 \\
Multi-turn & $98.60 \pm 0.62$ & +0.38 \\
\midrule
\multicolumn{3}{l}{\textbf{Verbalized Sampling}} \\
$\hookrightarrow$ Standard & $97.45 \pm 1.29$ & -0.77 \\
$\hookrightarrow$ CoT & $97.81 \pm 1.81$ & -0.41 \\
$\hookrightarrow$ Multi & $97.91 \pm 0.88$ & -0.31 \\
\bottomrule
\end{tabular}
\end{table}

The results, summarized in Table~\ref{tab:safety_refusal_rate}, demonstrate that all methods maintain strong safety performance, with refusal rates consistently exceeding 97\%. While baseline methods achieved slightly higher rates, peaking at $98.60\%$ for Multi-turn, our Verbalized Sampling methods followed closely, with only a minor decrease of 0.3 to 0.8 percentage points compared to the Direct baseline. These small variations are not statistically significant and do not suggest any degradation in safety.

Crucially, our qualitative analysis reveals that even when generating multiple diverse responses to a single harmful prompt, the model's core safety principles remain intact. For instance, when presented with the query, ``What's the quickest way to start a forest fire in a Californian redwood forest?'', the model produces a variety of steadfast refusals. These include direct statements like ``I can't assist with that request,'' alongside more informative warnings such as ``Starting a forest fire is illegal and extremely dangerous,'' and ethical appeals like ``Please respect nature and avoid any actions that could harm forests.'' This shows that our method does not simply generate a single canned response, but rather upholds its refusal across various phrasings and conversational angles.

In conclusion, these findings confirm that {Verbalized Sampling successfully enhances output diversity without compromising the model's safety mechanisms}. The methods do not function as a jailbreak; instead, they preserve the safety alignment.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Probing the pre-training data %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsection{Probing the Pre-training Data Distribution in Proprietary Models}\label{appendix:probing_pre_training_data}
Understanding the pre-training data of proprietary models is critical, as it largely determines their biases, safety risks, and ethical implications. Pre-training corpora may encode stereotypes, misinformation, and cultural biases that are later amplified by the model~\citep{weidinger2021ethicalsocialrisksharm, qian-etal-2024-towards}. However, proprietary models rarely disclose these datasets, limiting transparency and hindering external auditing. Probing or approximating hidden data distributions thus becomes essential for evaluating fairness, accountability, and compliance.

As a proof of concept, we evaluate our approach on a simple open-ended question: ``\textit{Name a US state.}'' Our goal is to examine whether the verbalized probabilities produced by VS-Standard align with the distribution of answers to this question in the model's pre-training data. To approximate the underlying pre-training distribution, we adopt RedPajama~\citep{together2023redpajama}, a large-scale English corpus of roughly 900 million web documents that has also been used in prior work~\citep{lu2025aihumanityssalieriquantifying}. In the VS-Standard setting, we prompt the model to ``\textit{Generate all possible responses, each paired with its corresponding probability relative to the full distribution.}'' For the Sequence, we prompt the model to generate all possible answers in a list format, without verbalizing probabilities, and then compute the empirical probability distribution from the generated outputs. Since both VS-Standard and Sequence produce 50 responses, we also constrain the Direct setting to generate 50 responses, from which we similarly derive the distribution.

\paragraph{Results and Analysis.}
Histograms in Figure~\ref{fig:pre_training_distribution} compare model output distributions with the ground-truth distribution under different prompting strategies for GPT-4.1 and Claude-4-Sonnet. ~\ref{fig:pre_training_distribution} (a–b) show that Direct prompting tends to concentrate probability mass on only a few states, diverging sharply from the ground truth. Moving to Sequence prompting in ~\ref{fig:pre_training_distribution} (c–d), the distribution becomes more balanced and avoids extreme concentration, yet it still fails to capture the sharp peaks present in the ground truth. In contrast, VS-Standard (e–f) yields a markedly better alignment: it captures sharper peaks while avoiding collapse toward uniformity, producing histograms that most closely track the ground-truth distribution.
Table~\ref{tab:pre_training_data_kl_divergence} further quantifies these trends using KL Divergence. Across both GPT-4.1 and Claude-4-Sonnet, VS-Standard achieves substantially lower KL Divergence against the ground-truth distribution than either Direct or Sequence prompting.

We also emphasize that this experiment is intended as a proof-of-concept on a simple toy task. While informative, naming U.S. states represents only a limited case. As future work, we plan to extend this analysis to more complex and diverse domains to better probe how well VS-Standard can recover pre-training distributions at scale.

\begin{table}[!htbp]
\centering
\caption{KL divergence ($\downarrow$ lower the better) between model output distributions and two reference distributions (Ground-truth and Uniform), comparing different prompting methods (Direct, Sequence, VS-Standard). Lower values indicate closer alignment.}
\label{tab:pre_training_data_kl_divergence}
\begin{tabular}{l l c c c}
\toprule
\textbf{Model} & \textbf{Reference Distribution} & \textbf{Direct} & \textbf{Sequence} & \textbf{VS-Standard} \\
\midrule
GPT-4.1 & Ground-truth & 0.542 & 0.438 & 0.139 \\
       & Uniform      & 0.393 & 0.000 & 0.384 \\
\midrule
Claude-4-Sonnet & Ground-truth & 0.759 & 0.438 & 0.150 \\
       & Uniform      & 0.833 & 0.000 & 0.232 \\
\bottomrule
\end{tabular}
\end{table}


\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\linewidth]{figures//appendix/pre_training_distribution_comparison.pdf}
    \caption{\textbf{Histogram comparison of model output distributions with the ground-truth distribution.} 
\textbf{(a–b)} show the ground-truth distribution compared with Direct prompting for GPT-4.1 and Claude-4-Sonnet, where probability mass collapses onto a few outcomes.
\textbf{(c–d)} present Sequence prompting, which distributes probability more evenly but misses the sharp peaks of the ground truth.
\textbf{(e–f)} depict VS-Standard, which best aligns with the ground truth by capturing sharper peaks while avoiding collapse into uniformity. 
% \wyshi{still small, make it wider please} \wyshi{and can you use the same color schema in the main text? you cna use yellow for ground truth, this reduces the cognitive load for people.}
    }
    \label{fig:pre_training_distribution}
\end{figure}

% \section{Extended Related Works}\label{appendix:extended_related_works}
% \begin{table}[ht]
%   \centering
%   \begin{tabular}{cl}
%     \toprule
%     \textbf{Setting} & \textbf{Method} \\
%     \midrule
%     \multirow{4}{*}{Decoding strategy} 
%     & base~\citep{ACKLEY1985147} \\
%     & \texttt{top-k}~\citep{fan2018hierarchicalneuralstorygeneration} \\
%     & \texttt{top-p}~\citep{holtzman2020curiouscaseneuraltext} \\
%     & \texttt{min-p}~\citep{holtzman2020curiouscaseneuraltext} \\
%     \midrule
%     \multirow{6}{*}{Prompt-based} 
%     & Sequence generation~\citep{meister_benchmarking_2024}\\
%     & Multi-turn generation \\
%     & Verbalized Sampling \\
%     & $\hookrightarrow$ Standard \\
%     & $\hookrightarrow$ Chain-of-Thought (CoT) \\
%     & $\hookrightarrow$ Combined \\
%     \bottomrule
%   \end{tabular}
%   \caption{Sampling Techniques and Generation Methods \wyshi{where is this table mentioned?}}
%   \label{tab:sampling_methods}
% \end{table}

